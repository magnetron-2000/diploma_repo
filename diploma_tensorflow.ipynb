{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f04fbf-b900-48bd-a2c1-6ea2895b26af",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6553d77d-0de9-4d79-bede-db30eb32a546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:15:51.022454: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-08 20:15:51.064861: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-08 20:15:51.065659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 20:15:53.021595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pylab as plt\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6865a-b923-4561-a319-242ed82b2ab4",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51392f27-7832-4cab-a3ea-0a7c755446af",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_BEST_ACCURACY_RESULTS = []\n",
    "SAVED_MODEL_BEST_LOSS_RESULTS = []\n",
    "IMG_SIZE = 64\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb70308-5f4f-4e97-a783-ddce4318c798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_and_transform(path):\n",
    "    data = utils.load_img(path, color_mode='grayscale', target_size=(64, 64))\n",
    "    data = utils.img_to_array(data)\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "    data = data / 255.0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f1fcef-0994-4627-b40f-3acdbdddfdca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(dataset_dir): # build 2 datasets for training and testing\n",
    "    x_train, y_train = [], []\n",
    "    classes = sorted(os.listdir(dataset_dir))\n",
    "    for i in range(len(classes)):\n",
    "        class_dir = os.path.join(dataset_dir, classes[i])\n",
    "        images = os.listdir(class_dir)\n",
    "        for image in images:\n",
    "            full_image_path = os.path.join(class_dir, image)\n",
    "            if 'ipynb_checkpoints' in full_image_path:\n",
    "                continue\n",
    "            x_train.append(get_image_and_transform(full_image_path))\n",
    "            y_train.append(i)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3393a6-e4e3-4d4d-9f97-6b82aa56a007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_examples_and_classes(dataset_dir, num): # return persons' names and persons' ears\n",
    "    classes = sorted(os.listdir(dataset_dir))\n",
    "    examples = []\n",
    "    for i in range(len(classes)):\n",
    "        class_dir = os.path.join(dataset_dir, classes[i])\n",
    "        images = os.listdir(class_dir)\n",
    "        examples.append(os.path.join(class_dir, images[num]))\n",
    "\n",
    "    return classes, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa81b474-0966-492b-8f25-eba67ab5cfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(accuracy, loss, val_accuracy, val_loss): # return schedule of accuracy and loss\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('epoch')\n",
    "\n",
    "    ax1.set_ylabel('total loss')\n",
    "    ax1.plot(loss,color='tab:red')\n",
    "    ax1.plot( val_loss, color='tab:brown')\n",
    "    ax1.tick_params(axis='y')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_ylabel('accuracy')  \n",
    "    ax2.plot( accuracy, color='tab:blue')\n",
    "    ax2.plot( val_accuracy, color='tab:green')\n",
    "    ax2.tick_params(axis='y')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71d1655-70cd-419b-b7cb-eff7a5a542c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_many(title='title', data=[]): # return schedules of accuracy\n",
    "    for i,o in enumerate(data):\n",
    "        plt.figure(i)\n",
    "        plt.plot([i for i in range(EPOCHS)],o, label=title)\n",
    "        plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab19927f-1208-4bfd-a329-933b6a21a2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def max_or_min_avarage_val(list_of_data, accuracy=True): #return dictionary of average values of lists\n",
    "    dict_all_l = {}\n",
    "    for i, o in enumerate(list_of_data):\n",
    "        dict_all_l[i] = sum(o)/len(o)\n",
    "    if accuracy:\n",
    "        return dict(sorted(dict_all_l.items(), key=lambda item: item[1], reverse=True))\n",
    "    else:\n",
    "        return dict(sorted(dict_all_l.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253f7b60-36bb-494f-9a81-078cc84b280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lists(): # save lists of accuracy and loss\n",
    "    t = time.ctime(time.time())[:-5]\n",
    "    with open(f'accuracy {t}.txt', 'w') as f:\n",
    "        for item in SAVED_MODEL_BEST_ACCURACY_RESULTS:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    # with open(f'loss {t}.txt', 'w') as f:\n",
    "    #     for item in SAVED_MODEL_BEST_LOSS_RESULTS:\n",
    "    #         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d59b4e0-8d86-4f58-a171-392fe5498f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(filename): # read accuracy and loss data  from files\n",
    "    lst = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            if line in ('', ' ','\\n'):\n",
    "                break\n",
    "            line = line[1:-2].split(', ')\n",
    "            lst.append([float(i.strip()) for i in line])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9c0e79-cdeb-4c1a-9cfb-ec1fcd518589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_best_res(list_of_data): # return max value in every list of accuracy\n",
    "    max_val = {}\n",
    "    for i, o in enumerate(list_of_data):\n",
    "        max_val[i] = max(o)\n",
    "    return dict(sorted(max_val.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198520c-7008-4b6c-b7dd-bc94f7fc2f08",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ed47a4-8c69-48d5-af81-e6d5f87955d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder = \"/home/jovyan/dataset/dataset2/train\"\n",
    "test_folder = \"/home/jovyan/dataset/dataset2/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7138b14b-26ee-4297-9c9b-2c552d9d6c53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22736 images belonging to 164 classes.\n",
      "Found 5683 images belonging to 164 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()#rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator()#rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_folder,\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 color_mode='grayscale')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_folder,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 16,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            color_mode='grayscale')\n",
    "\n",
    "# print(training_set.filepaths)\n",
    "# print(training_set.classes)\n",
    "# print(training_set.class_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0f1f8-a05a-47b6-8962-711522d51998",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2d5c7a-7d38-463b-ac3e-19033eaef6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 64, 64, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 32, 32, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 32, 32, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 32, 32, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 16, 16, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 16, 16, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 16, 16, 8)         264       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 8, 8, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 8, 8, 8)          32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 8)           264       \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 4, 4, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 4, 4, 8)          32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 164)               84132     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151,500\n",
      "Trainable params: 151,436\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model(in_1=8, in_2=8, in_3=8, in_4=8):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=in_1,kernel_size=3, strides=(1,1), input_shape=(64,64,1), padding='same', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "\n",
    "    model.add(Conv2D(filters=in_2,kernel_size=3, strides=1, padding = 'same', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=in_3,kernel_size=2, strides=1, padding ='same', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(filters=in_4,kernel_size=2, strides=1, padding ='same', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(Dense(units = 512 , activation = 'tanh'))\n",
    "    # model.add(Dense(units = 1))\n",
    "    model.add(Dense(units = 164 , activation = 'softmax'))\n",
    "\n",
    "    model.compile( optimizer = 'Adam', loss = 'categorical_crossentropy',metrics = ['accuracy']) # optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    return model\n",
    "summary_example = model()\n",
    "summary_example.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfdf6c4-bed3-4b6f-92bd-9a2e9c16403a",
   "metadata": {},
   "source": [
    "choose best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820b32f0-c987-41b1-bf6a-f268b1675ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_and_best_hyperparameters(nums):\n",
    "    start_time1 = time.time() \n",
    "    for i in nums:\n",
    "        start_time2 = time.time()\n",
    "        \n",
    "        ready_model = model(i,i*2,i*3, i*4)        # change order \n",
    "        trained_model = ready_model.fit(training_set, steps_per_epoch = 32, epochs = EPOCHS, validation_data = test_set)\n",
    "        \n",
    "        print(\"for one training time taken: {:.2f} seconds\".format(time.time() - start_time2)) # how much time one training take\n",
    "        \n",
    "        accuracy = trained_model.history['accuracy']\n",
    "        loss = trained_model.history['loss']\n",
    "\n",
    "        SAVED_MODEL_BEST_ACCURACY_RESULTS.append(accuracy)\n",
    "        SAVED_MODEL_BEST_LOSS_RESULTS.append(loss)\n",
    "        \n",
    "    print(\"for all training time taken: {:.2f} seconds\".format(time.time() - start_time1)) # how much time all training take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa15b49-da5a-4cbb-bf6a-0fadef1326ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:18:42.182383: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 0s - loss: 5.3529 - accuracy: 0.0186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:18:45.826146: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 9s 234ms/step - loss: 5.3529 - accuracy: 0.0186 - val_loss: 5.5004 - val_accuracy: 0.0065\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 5.1267 - accuracy: 0.0303 - val_loss: 5.3333 - val_accuracy: 0.0125\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 6s 200ms/step - loss: 4.9869 - accuracy: 0.0391 - val_loss: 5.2824 - val_accuracy: 0.0151\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 7s 235ms/step - loss: 4.8379 - accuracy: 0.0654 - val_loss: 4.9579 - val_accuracy: 0.0310\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 4.6993 - accuracy: 0.0674 - val_loss: 4.8629 - val_accuracy: 0.0377\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 4.5379 - accuracy: 0.0801 - val_loss: 4.9350 - val_accuracy: 0.0399\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 4.5584 - accuracy: 0.0781 - val_loss: 4.8561 - val_accuracy: 0.0475\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 4.4298 - accuracy: 0.0869 - val_loss: 4.8291 - val_accuracy: 0.0487\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 4.2371 - accuracy: 0.1240 - val_loss: 4.5208 - val_accuracy: 0.0721\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 7s 234ms/step - loss: 4.1812 - accuracy: 0.1338 - val_loss: 4.8914 - val_accuracy: 0.0544\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 6s 188ms/step - loss: 4.0228 - accuracy: 0.1387 - val_loss: 4.6809 - val_accuracy: 0.0660\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 6s 191ms/step - loss: 3.9959 - accuracy: 0.1504 - val_loss: 4.4087 - val_accuracy: 0.0952\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 3.8861 - accuracy: 0.1768 - val_loss: 4.4295 - val_accuracy: 0.0954\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 7s 236ms/step - loss: 3.8176 - accuracy: 0.1895 - val_loss: 4.1752 - val_accuracy: 0.1239\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 6s 190ms/step - loss: 3.7725 - accuracy: 0.2012 - val_loss: 4.1696 - val_accuracy: 0.1184\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 3.5833 - accuracy: 0.2197 - val_loss: 4.2493 - val_accuracy: 0.1228\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 3.5622 - accuracy: 0.2354 - val_loss: 4.0787 - val_accuracy: 0.1316\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 3.3819 - accuracy: 0.2676 - val_loss: 4.1225 - val_accuracy: 0.1351\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 3.4091 - accuracy: 0.2539 - val_loss: 4.6633 - val_accuracy: 0.0862\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 3.3516 - accuracy: 0.2812 - val_loss: 3.9722 - val_accuracy: 0.1501\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 7s 214ms/step - loss: 3.2631 - accuracy: 0.2861 - val_loss: 3.9099 - val_accuracy: 0.1624\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 3.2232 - accuracy: 0.2998 - val_loss: 4.2028 - val_accuracy: 0.1369\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 3.0790 - accuracy: 0.3174 - val_loss: 3.7887 - val_accuracy: 0.1739\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 3.1241 - accuracy: 0.3096 - val_loss: 4.1548 - val_accuracy: 0.1399\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 2.9904 - accuracy: 0.3418 - val_loss: 4.1144 - val_accuracy: 0.1480\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 2.9828 - accuracy: 0.3340 - val_loss: 3.7773 - val_accuracy: 0.1848\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 2.8650 - accuracy: 0.3721 - val_loss: 3.9059 - val_accuracy: 0.1749\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 6s 196ms/step - loss: 2.7110 - accuracy: 0.3799 - val_loss: 3.7964 - val_accuracy: 0.1929\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 7s 234ms/step - loss: 2.8089 - accuracy: 0.3750 - val_loss: 3.6140 - val_accuracy: 0.2141\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 7s 235ms/step - loss: 2.6733 - accuracy: 0.4033 - val_loss: 3.6121 - val_accuracy: 0.2119\n",
      "for one training time taken: 211.20 seconds\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:22:13.372402: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 0s - loss: 5.5021 - accuracy: 0.0195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:22:19.439347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 14s 414ms/step - loss: 5.5021 - accuracy: 0.0195 - val_loss: 5.7314 - val_accuracy: 0.0083\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 5.1950 - accuracy: 0.0273 - val_loss: 5.4354 - val_accuracy: 0.0148\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 5.0346 - accuracy: 0.0371 - val_loss: 5.3360 - val_accuracy: 0.0157\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 10s 332ms/step - loss: 4.8986 - accuracy: 0.0518 - val_loss: 4.9061 - val_accuracy: 0.0391\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 10s 327ms/step - loss: 4.7607 - accuracy: 0.0645 - val_loss: 5.3730 - val_accuracy: 0.0183\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 11s 358ms/step - loss: 4.6403 - accuracy: 0.0742 - val_loss: 4.9192 - val_accuracy: 0.0408\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 13s 413ms/step - loss: 4.4242 - accuracy: 0.0947 - val_loss: 5.1190 - val_accuracy: 0.0373\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 13s 408ms/step - loss: 4.3576 - accuracy: 0.0898 - val_loss: 4.5320 - val_accuracy: 0.0697\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 11s 361ms/step - loss: 4.3164 - accuracy: 0.1045 - val_loss: 4.6082 - val_accuracy: 0.0600\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 4.2009 - accuracy: 0.1123 - val_loss: 4.5072 - val_accuracy: 0.0750\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 13s 410ms/step - loss: 4.0645 - accuracy: 0.1279 - val_loss: 4.3628 - val_accuracy: 0.0910\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 13s 406ms/step - loss: 3.9443 - accuracy: 0.1572 - val_loss: 5.0057 - val_accuracy: 0.0516\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 3.9023 - accuracy: 0.1611 - val_loss: 4.4342 - val_accuracy: 0.0904\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 3.8096 - accuracy: 0.1709 - val_loss: 4.4939 - val_accuracy: 0.0892\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 13s 405ms/step - loss: 3.6317 - accuracy: 0.2168 - val_loss: 5.0277 - val_accuracy: 0.0554\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 3.5307 - accuracy: 0.2080 - val_loss: 4.6094 - val_accuracy: 0.0707\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 3.5170 - accuracy: 0.2451 - val_loss: 4.1295 - val_accuracy: 0.1179\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 13s 411ms/step - loss: 3.3327 - accuracy: 0.2588 - val_loss: 3.9241 - val_accuracy: 0.1559\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 3.3997 - accuracy: 0.2529 - val_loss: 4.1050 - val_accuracy: 0.1360\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 3.3129 - accuracy: 0.2656 - val_loss: 3.8627 - val_accuracy: 0.1663\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 13s 406ms/step - loss: 3.1805 - accuracy: 0.2920 - val_loss: 4.0308 - val_accuracy: 0.1501\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 13s 401ms/step - loss: 3.1414 - accuracy: 0.3008 - val_loss: 3.9572 - val_accuracy: 0.1557\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 2.9624 - accuracy: 0.3213 - val_loss: 4.0433 - val_accuracy: 0.1431\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 13s 406ms/step - loss: 2.9748 - accuracy: 0.3262 - val_loss: 4.0092 - val_accuracy: 0.1554\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 3.0084 - accuracy: 0.2988 - val_loss: 3.5797 - val_accuracy: 0.2046\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 13s 403ms/step - loss: 2.7397 - accuracy: 0.3799 - val_loss: 3.5613 - val_accuracy: 0.2166\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 2.6384 - accuracy: 0.3955 - val_loss: 3.6160 - val_accuracy: 0.2101\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 13s 403ms/step - loss: 2.6663 - accuracy: 0.4014 - val_loss: 3.5133 - val_accuracy: 0.2245\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 13s 408ms/step - loss: 2.7872 - accuracy: 0.3760 - val_loss: 3.6472 - val_accuracy: 0.2039\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 2.5779 - accuracy: 0.4023 - val_loss: 3.4941 - val_accuracy: 0.2240\n",
      "for one training time taken: 374.98 seconds\n",
      "for all training time taken: 586.18 seconds\n"
     ]
    }
   ],
   "source": [
    "model_and_best_hyperparameters([16,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e05ca7b-7539-48ec-92a6-7a7f88d8a982",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted list of average  epochs' values for accuracy   {0: 0.20856119791666666, 1: 0.2078125}\n",
      "sorted list of max  epochs' values for accuracy   {0: 0.4033203125, 1: 0.40234375}\n"
     ]
    }
   ],
   "source": [
    "print(\"sorted list of average  epochs' values for accuracy  \", max_or_min_avarage_val(SAVED_MODEL_BEST_ACCURACY_RESULTS))\n",
    "print(\"sorted list of max  epochs' values for accuracy  \", show_best_res(SAVED_MODEL_BEST_ACCURACY_RESULTS))\n",
    "# save_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67179992-f15d-432b-8e32-45e179ce34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = 'accuracy Sat May  6 11:19:23.txt'\n",
    "# l = read_list_from_file(file)\n",
    "# print(\"sorted list of max  epochs' values for accuracy  \", show_best_res(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b49b6d-64b7-4aa9-b92e-f71dfeeb6f00",
   "metadata": {},
   "source": [
    "train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a3702-1baf-4dab-a09c-3dee963376fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_model():\n",
    "    ready_model = model(8,16,32,64)\n",
    "    call_back = ModelCheckpoint(f\"/home/jovyan/weights.hdf5\", monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    trained_model = ready_model.fit(training_set, steps_per_epoch = 32, epochs = 30, validation_data = test_set, callbacks = [call_back])\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d089bf-cdf6-466c-b982-050830bc5c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_model = final_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd227c-d4d8-4b5d-80fe-3f7959fa5aa4",
   "metadata": {},
   "source": [
    "plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d3b27-a90b-455a-93cb-59328ed8fb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_many('accuracy', SAVED_MODEL_BEST_ACCURACY_RESULTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a59931-66c2-4806-a777-e4bfee1cad8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if trained_model:\n",
    "    plot(trained_model.history['accuracy'], trained_model.history['loss'], trained_model.history['val_accuracy'], trained_model.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8f3f1-649e-452d-922b-fa70c8ce7d92",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc123d2-9d89-44f2-989e-5d4c4d4bedc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_model = Sequential()\n",
    "# for layer in model.layers[:-1]:\n",
    "#     predict_model.add(layer)\n",
    "# predict_model.add(Dense(units=1, activation=None))\n",
    "# predict_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# predict_model.summary()\n",
    "\n",
    "predict_model = model(8,16,32,64)\n",
    "predict_model.summary()\n",
    "predict_model.load_weights('weights.hdf5')#, skip_mismatch=True, by_name=True)\n",
    "\n",
    "# # Load weights and biases from model manually\n",
    "# for i in range(len(predict_model.layers) - 1):\n",
    "#     predict_model.layers[i].set_weights(model.layers[i].get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0dd9f-d192-4a8a-83a3-a97d19effe0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, known_data_path, unknown_data_path):\n",
    "    known_data = get_image_and_transform(known_data_path)\n",
    "    unknown_data = get_image_and_transform(unknown_data_path)\n",
    "\n",
    "    known_data = model.predict(known_data)\n",
    "    unknown_data = model.predict(unknown_data)\n",
    "    distance = unknown_data - known_data\n",
    "\n",
    "    threshold = 0.018614814\n",
    "    return  distance[0][0] < threshold, distance[0][0]\n",
    "\n",
    "predict(predict_model, '/home/jovyan/12.jpg', '/home/jovyan/11.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed7301-057b-4f6a-9569-c220b9e72a18",
   "metadata": {
    "tags": []
   },
   "source": [
    "plot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b9c60-bc0c-4586-8fc2-4eb86d972c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes, examples1 = get_examples_and_classes(test_folder,1)\n",
    "classes, examples2 = get_examples_and_classes(test_folder,2)\n",
    "y_true = [i for i in range(len(classes))]\n",
    "y_predict_tensors = []\n",
    "y_predict_labels = []\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    predict_label, predict_tensor = predict(predict_model, examples1[i], examples2[i])\n",
    "    y_predict_tensors.append(predict_tensor)\n",
    "    y_predict_labels.append(predict_label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292a756-9bbe-4a08-8a58-9d30b170abb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_predict_labels, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71eed0-463d-4c6b-9b8e-baf435aa25ce",
   "metadata": {},
   "source": [
    "choose best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094515cc-bc73-4f5c-8a6b-195dc45eadd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.0001, 0.5, 1000)\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_predict_tensors < threshold).astype(int)\n",
    "    f1_scores.append(f1_score(y_true, y_pred, average='micro'))\n",
    "\n",
    "# Choose the threshold that maximizes F1 score\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
